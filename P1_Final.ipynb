{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P1 I use a CNN with the following key aspects:\n",
    "\n",
    "- A CNN architecture with three convolutional layers, each followed by max pooling and optional dropout.\n",
    "- Includes a fully connected layer with optional dropout before the final classification layer\n",
    "- Trains the model using five different optimizers: SGD, AdaGrad, RMSProp, Nesterov, and Adam\n",
    "- For each optimizer, tracks training loss, training accuracy, and validation accuracy over 100 epochs\n",
    "- Uses a fixed learning rate of 0.001 and limits each epoch to 50 mini-batches with batch size of 128\n",
    "- Implements a function to create models with configurable dropout rates, though in the final execution it sets dropout_rate=0\n",
    "- Shuffles the training data before each epoch to improve training performance\n",
    "\n",
    "\n",
    "There are 3 parts to my solution. \n",
    "\n",
    "- First I run all 5 optimizers without any dropout.\n",
    "- Then I repeat with a dropout of 0.25 for all 5 optimizers. \n",
    "- Finally as an extension I pick the 2 best optimizers (in both validation accuracy and training loss) - Adam and RMSProp and vary dropout values from 0.1-0.3 to gauge the effects of dropout on training loss. \n",
    "- I discuss the outputs in the pdf attached to my solution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um9hrvYnfIss",
    "outputId": "9aacc555-7e52-4e27-ee55-3e15536271f0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()  # Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(data_train, label_train), (data_test, label_test) = cifar10.load_data()\n",
    "data_train, data_test = data_train / 255.0, data_test / 255.0  # Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESOiGKdQfihi"
   },
   "outputs": [],
   "source": [
    "def create_cnn_dropout(dropout_rate=0.25):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),  # Added dropout\n",
    "\n",
    "        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),  # Added dropout\n",
    "\n",
    "        layers.Conv2D(128, (5, 5), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        Dropout(dropout_rate),  # Added dropout\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1000, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Added dropout\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_model(optimizer, dropout_rate, num_epochs=100, batch_size=128, validation_data=None):\n",
    "    model = create_cnn_dropout(dropout_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Initialize lists to store losses, training accuracies, and validation accuracies\n",
    "    losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    num_batches = 50  # Limit to 50 mini-batches per epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.arange(len(data_train))\n",
    "        np.random.shuffle(indices)\n",
    "        data_train_shuffled = data_train[indices]\n",
    "        label_train_shuffled = label_train[indices]\n",
    "\n",
    "        epoch_loss = []\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_x = data_train_shuffled[start:end]\n",
    "            batch_y = label_train_shuffled[start:end]\n",
    "\n",
    "            loss, accuracy = model.train_on_batch(batch_x, batch_y)\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "            # Track training accuracy\n",
    "            correct_predictions += accuracy * batch_size\n",
    "            total_predictions += batch_size\n",
    "\n",
    "        # Compute the average loss and training accuracy for this epoch\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        train_accuracies.append(correct_predictions / total_predictions)\n",
    "\n",
    "        # Track validation accuracy\n",
    "        val_loss, val_accuracy = model.evaluate(validation_data[0], validation_data[1], verbose=0)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "    return losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 5 optimizers without dropout\n",
    "\n",
    "optimizers_dict = {\n",
    "    \"SGD\": optimizers.SGD(learning_rate=0.001),\n",
    "    \"AdaGrad\": optimizers.Adagrad(learning_rate=0.001),\n",
    "    \"RMSProp\": optimizers.RMSprop(learning_rate=0.001),\n",
    "    \"Nesterov\": optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True),\n",
    "    \"Adam\": optimizers.Adam(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "validation_data = (data_test, label_test)  # Validation set\n",
    "\n",
    "# Train models and store losses, accuracies\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "results_dict = {}\n",
    "\n",
    "for opt_name, opt in optimizers_dict.items():\n",
    "    print(f\"Training with {opt_name}...\")\n",
    "    losses, train_accuracies, val_accuracies = train_model(opt, dropout_rate=0, num_epochs=num_epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "    results_dict[opt_name] = {'losses': losses, 'train_accuracies': train_accuracies, 'val_accuracies': val_accuracies}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xtAwXtb8fmjF",
    "outputId": "bb5a9db3-a9af-4063-d98f-a77018f4b2c3"
   },
   "outputs": [],
   "source": [
    "# All 5 optimizers with dropout rates\n",
    "\n",
    "optimizers_dict = {\n",
    "    \"SGD\": optimizers.SGD(learning_rate=0.001),\n",
    "    \"AdaGrad\": optimizers.Adagrad(learning_rate=0.001),\n",
    "    \"RMSProp\": optimizers.RMSprop(learning_rate=0.001),\n",
    "    \"Nesterov\": optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True),\n",
    "    \"Adam\": optimizers.Adam(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "validation_data = (data_test, label_test)  # Validation set\n",
    "\n",
    "# Train models and store losses, accuracies\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "results_dict = {}\n",
    "\n",
    "for opt_name, opt in optimizers_dict.items():\n",
    "    print(f\"Training with {opt_name}...\")\n",
    "    losses, train_accuracies, val_accuracies = train_model(opt, dropout_rate=0.25, num_epochs=num_epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "    results_dict[opt_name] = {'losses': losses, 'train_accuracies': train_accuracies, 'val_accuracies': val_accuracies}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvGoN6VL-HA-"
   },
   "outputs": [],
   "source": [
    "# Extract training losses from the results\n",
    "losses_dict = {}\n",
    "\n",
    "for opt_name, result in results_dict.items():\n",
    "    losses_dict[opt_name] = result['losses']\n",
    "\n",
    "# Plotting training losses for different optimizers\n",
    "plt.figure(figsize=(10, 6))\n",
    "for opt_name, losses in losses_dict.items():\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label=opt_name)\n",
    "\n",
    "plt.title('Training Loss vs Epochs for Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Extract train and validation accuracies from the results\n",
    "train_accuracies_dict = {}\n",
    "val_accuracies_dict = {}\n",
    "\n",
    "for opt_name, result in results_dict.items():\n",
    "    train_accuracies_dict[opt_name] = result['train_accuracies']\n",
    "    val_accuracies_dict[opt_name] = result['val_accuracies']\n",
    "\n",
    "# Plotting train accuracies for different optimizers\n",
    "plt.figure(figsize=(10, 6))\n",
    "for opt_name, train_accuracies in train_accuracies_dict.items():\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label=opt_name)\n",
    "\n",
    "plt.title('Train Accuracy vs Epochs for Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting validation accuracies for different optimizers\n",
    "plt.figure(figsize=(10, 6))\n",
    "for opt_name, val_accuracies in val_accuracies_dict.items():\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracies, label=opt_name)\n",
    "\n",
    "plt.title('Validation Accuracy vs Epochs for Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhZgTwAf_p6j"
   },
   "outputs": [],
   "source": [
    "#Varying dropout values for Adam and RMSProp\n",
    "\n",
    "optimizers_dict = {\n",
    "    \"Adam\": optimizers.Adam(learning_rate=0.001),\n",
    "    \"RMSProp\": optimizers.RMSprop(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "dropout_values = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "# Train models and store losses\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "losses_dict = {}\n",
    "\n",
    "for opt_name, optimizer_class in [(\"Adam\", Adam), (\"RMSprop\", RMSprop)]:\n",
    "    for dropout in [0.1, 0.15, 0.2, 0.25, 0.3]:\n",
    "        print(f\"Training with {opt_name} optimizer and dropout {dropout}...\")\n",
    "        key = f\"{opt_name}_dropout_{dropout}\"\n",
    "        opt = optimizer_class()\n",
    "        losses_dict[key] = train_model(opt, dropout, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D730mEnaAzrV"
   },
   "outputs": [],
   "source": [
    "# Plot Adam and RMSProp with 5 different dropout values\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# First chart: Adam optimizer with different dropout rates\n",
    "plt.subplot(1, 2, 1)\n",
    "dropout_rates = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    losses = losses_dict[f\"Adam_dropout_{dropout}\"]  # Get loss values\n",
    "    plt.plot(range(len(losses)), losses, marker='o', label=f\"Dropout {dropout}\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Adam Optimizer - Training Loss vs Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Second chart: RMSprop optimizer with different dropout rates\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for dropout in dropout_rates:\n",
    "    losses = losses_dict[f\"RMSprop_dropout_{dropout}\"]  # Get loss values\n",
    "    plt.plot(range(len(losses)), losses, marker='o', label=f\"Dropout {dropout}\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"RMSprop Optimizer - Training Loss vs Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
